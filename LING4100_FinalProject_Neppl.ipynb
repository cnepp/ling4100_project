{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caitlin Neppl  \n",
    "LING 4100: Machine Learning in Linguistics  \n",
    "Fall 2019  \n",
    "Prof Mans Hulden  \n",
    "Final Project  \n",
    "\n",
    "Character Classification using Stochastic Gradient Descent Training\n",
    "==========\n",
    "\n",
    "\n",
    "**-- Data --**\n",
    "\n",
    "The webcomic _Homestuck_ (2009-2016) is a story that begins with four human kids, who are online friends. They communicate to one another throughout the comic via the \"Pesterchum\" instant messaging platform. The comic is known for being lengthy (at ~ 900,000 words) with much of the wordcount being taken up by these \"pesterlogs\". \n",
    "<img src= \"pesterlog.png\" alt=\"Pesterlog\" width=\"600\" height=\"200\">\n",
    "\n",
    "Many of the characters in the comic have very particular ways of typing (it's an alien thing), which helps to differentiate them from one another, which is why I initially thought of this project. However, it was much simpler to stick to the human kids, as their data would be a lot easier to parse and compare with one another. I thought it would be interesting to see how different their vocabularies are such that a machine learning strategy would enable high accuracy in predictions, and to explore what kinds of multi-class classifiers were out there and see how they worked.\n",
    "\n",
    "The text file that contained all of the text from the webcomic was obtained courtesy of http://readmspa.org/. I was able to extract the lines from the pesterlogs of the four main characters. \n",
    "\n",
    "As such, my classifier is composed of  four classes, one for each character: Dave Strider, John Egbert, Jade Harley, and Rose Lalonde. Examples are composed of a character's dialog from a single pesterlog exchange. Features are words. My initial goal was to train a classifier using the data from the earlier parts of the comic, and test it on the newer ones (when they're older), but I ended up being surprised by how much data I would need to make the classifier work decently.  \n",
    "\n",
    "\n",
    "There are many multi-class classifier models out there, but I landed on this Stochastic Gradient Descent Model, in part because it is well-supported through the Scikit-learn library, which I utilized on this project. Prior to classification, the data was processed using NLTK and other Python techniques. Sklearn was then used to format the data from txt to matrix, and stop words were removed. Weights were adjusted using TF-IDF, and I used Sklearn's linear model library's SGD training for the classifier. \n",
    "\n",
    "_(Note:)_ The cleaning code below is kind of wonky since I changed the file structure as I figured out how to handle the data. The fully processed example files are up on my github: https://github.com/cnepp/ling4100_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def writeFile(filename, text):\n",
    "     # create file if not exists, write to eof\n",
    "    f = open(filename, 'a+', encoding='utf8')\n",
    "    for i in text:\n",
    "        f.write(i)\n",
    "    f.close()\n",
    "    print(\"file written to:\", filename)\n",
    "\n",
    "# separate character lines from entire script\n",
    "file = open('hsscrpt.txt', 'rt', encoding='utf8')\n",
    "\n",
    "count = 0 # counter for keeping track of which example\n",
    "act6 = False\n",
    "\n",
    "#iterate through all lines in file\n",
    "for line in file:\n",
    "    \n",
    "    # conditions for acts 1-5\n",
    "    ds_conditions = (\"DAVE: \" in line) or (\"TG: \" in line) or (\"DAVESPRITE: \" in line)\n",
    "    je_conditions = (\"JOHN: \" in line) or (\"EB: \" in line) or (\"GT: \" in line)\n",
    "    jh_conditions = (\"JADE: \" in line) or (\"GG:\" in line) or (\"JADESPRITE: \" in line)\n",
    "    rl_conditions = (\"ROSE: \" in line) or (\"TT: \" in line)\n",
    "    \n",
    "    # need to change conditions once we enter act 6 - handle entanglement (eg roxy and dave are both'TG' )\n",
    "    if \"[S] ACT 6\" in line: \n",
    "        act6 = True\n",
    "    if act6 == True:\n",
    "        ds_conditions = (\"DAVE: \" in line) or (\"DAVESPRITE: \" in line)\n",
    "        je_conditions = (\"JOHN: \" in line) or (\"EB: \" in line)\n",
    "        jh_conditions = (\"JADE: \" in line) or (\"JADESPRITE: \" in line)\n",
    "        rl_conditions = (\"ROSE: \" in line)\n",
    "\n",
    "    if (\"pesterlog\" in line) or (\"spritelog\" in line) or (\"dialoglog\" in line): # indicates beginning of convo\n",
    "        count = count + 1 \n",
    "    \n",
    "    # dave\n",
    "    if ds_conditions: \n",
    "        dspath = 'examples/ds/ds_' + str(count) + '.txt'\n",
    "        #writeFile(dspath, line)\n",
    "   \n",
    "    # john\n",
    "    if je_conditions:\n",
    "        path = 'examples/je/je_' + str(count) + '.txt'\n",
    "        #writeFile(path, line)\n",
    "        \n",
    "    # jade\n",
    "    if jh_conditions:\n",
    "        path = 'examples/jh/jh_' + str(count) + '.txt'\n",
    "        #writeFile(path, line)\n",
    "        \n",
    "    # rose\n",
    "    if rl_conditions:\n",
    "        path = 'examples/rl/rl_' + str(count) + '.txt'\n",
    "        #writeFile(path, line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n",
    "\n",
    "* Used NLTK to isolate tokens and clean them up\n",
    "* Cleaned by:\n",
    "    * tokinizing using TwitterTokenizer\n",
    "    * all lowercase\n",
    "    * no punctuation\n",
    "    * no non-alpha characters\n",
    "    * removed chat handles / names which prefaced each line \n",
    "* Stored character info/previously separated files in dict\n",
    "* Wrote cleaned data for each example into a separate file \n",
    "\n",
    " Four classes: dave, john, rose, jade  \n",
    " Example is one line  \n",
    " Features are words in the line (out of some crazy total)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, os\n",
    "\n",
    "# dictionaries for ea character to make data processing a lil easier\n",
    "\n",
    "# dave strider (CLASS 0)\n",
    "daveStrider = {\n",
    "    'examples': 'raw_examples/ds/dirt/',\n",
    "    'initials' : 'ds',\n",
    "    'name' : 'dave',\n",
    "    'handle' : 'tg',\n",
    "    'aka' : 'davesprite'\n",
    "}\n",
    "# john egbert (CLASS 1)\n",
    "johnEgbert = {\n",
    "    'examples': 'raw_examples/je/dirt/',\n",
    "    'initials' : 'je',\n",
    "    'name' : 'john',\n",
    "    'handle' : 'eb',\n",
    "    'aka' : 'gt'\n",
    "}\n",
    "# jade harley (CLASS 2)\n",
    "jadeHarley = {\n",
    "    'examples': 'raw_examples/jh/dirt/',\n",
    "    'initials' : 'jh',\n",
    "    'name' : 'jade',\n",
    "    'handle' : 'gg',\n",
    "    'aka' : 'jadesprite'\n",
    "}\n",
    "# rose lalonde (CLASS 3)\n",
    "roseLalonde = {\n",
    "    'examples': 'raw_examples/rl/dirt/',\n",
    "    'initials' : 'rl',\n",
    "    'name' : 'rose',\n",
    "    'handle' : 'tt',\n",
    "    'aka' : ''\n",
    "}\n",
    "\n",
    "# helper functions to clean up data\n",
    "\n",
    "def readFile(filename):\n",
    "    file = open(filename, 'rt')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "    \n",
    "def tokenize(pesterlog):\n",
    "    # TweekTokenizer much more forgiving about punctuation  \n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    tt = TweetTokenizer()\n",
    "    tokens = tt.tokenize(pesterlog)\n",
    "    return tokens\n",
    "\n",
    "def lowercase(tokens):\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    return tokens\n",
    "\n",
    "def punctuation(tokens):\n",
    "    punct = '!#$%&\"()*+,-./:;?' \n",
    "    punct = punct + \"'\" # gotta cram that apostophe in there somehow\n",
    "    table = str.maketrans('', '', punct)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    return stripped\n",
    "\n",
    "def removeNonAlpha(strippedwords):\n",
    "    # remove remaining tokens (containing) non-alphabetic characters\n",
    "    words = [word for word in strippedwords if word.isalpha()]\n",
    "    return words\n",
    "\n",
    "def removeWords(name, handle, aka, words):    \n",
    "    # remove handles/names\n",
    "    handles = set([name, handle, aka])\n",
    "    words = [w for w in words if (not w in handles)]\n",
    "    return words\n",
    "\n",
    "def writeWords(filename, words):\n",
    "    # create if not exists; overwrite if exists\n",
    "    f = open(filename, 'w+', encoding = 'utf8')\n",
    "    for i in words:\n",
    "        f.write(i +'\\n')\n",
    "    f.close()\n",
    "    print(\"file written to \", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(kid):\n",
    "    print(kid['name'])\n",
    "    #wordcount = 0\n",
    "    freq = {}\n",
    "    charwords = []\n",
    "\n",
    "    # for all examples (pesterlogs)\n",
    "    for file in os.listdir(kid['examples']):\n",
    "        # run through helper functions for each component\n",
    "        filename = kid['examples'] + file\n",
    "        #print(\"reading in \", filename)\n",
    "        rawtext = readFile(filename)\n",
    "        tokens = tokenize(rawtext) \n",
    "        tokens = lowercase(tokens)  \n",
    "        tokens = punctuation(tokens)\n",
    "        tokens = removeNonAlpha(tokens)        \n",
    "        words = removeWords(kid['name'], kid['handle'], kid['aka'], tokens)\n",
    "        \n",
    "        # put into a nice list for later tinkering in scikit or whatever\n",
    "        listToStr = ' '.join([str(elem) for elem in words]) \n",
    "        charwords.append(listToStr) \n",
    "        \n",
    "        # get word frequencies\n",
    "        for i in words:\n",
    "            freq[i] = freq.get(i,0) + 1\n",
    "        #wordcount += len(words)\n",
    "        \n",
    "        # write new file with cleaned up tokens\n",
    "        nfile = 'examples/' + kid['initials'] + '/clean/clean_' + file # format stupid path\n",
    "        #writeWords(nfile, words) \n",
    "        #print(words[:100])\n",
    "    \n",
    "    #print(\"wordcount\", wordcount, kid['name'])   \n",
    "    wordfreq = sorted(freq.items(), key = lambda x: x[1], reverse = True)\n",
    "    print(len(wordfreq))\n",
    "    #return charwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'daveStrider' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-34f9d86121f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# generate cleaned up word files for each kid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mchars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdaveStrider\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjohnEgbert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroseLalonde\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjadeHarley\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m      \u001b[0mcleanData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'daveStrider' is not defined"
     ]
    }
   ],
   "source": [
    "# generate cleaned up word files for each kid\n",
    "chars = [daveStrider, johnEgbert, roseLalonde, jadeHarley]\n",
    "for c in chars:\n",
    "     cleanData(c)\n",
    "        \n",
    "# array format, one ex = one entry \n",
    "dswords = cleanData(daveStrider)\n",
    "jewords = cleanData(johnEgbert)\n",
    "jhwords = cleanData(jadeHarley)\n",
    "rlwords = cleanData(roseLalonde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "\n",
    "\n",
    "Character            |  Words     |    Acts 1-4      |     Act 5        |  Act 6          | Unique Words  |   Examples\n",
    " ------------------- |------------|------------------| ---------------  | ----------------|---------------|------------\n",
    "    Dave Strider     |   48734    |  9425(~19%)    |   15319(~31%)  |  23990(~50%)  |    5443       |     371\n",
    "    John Egbert      |   46684    |  8538(~18%)    |   14090(~30%)  |  24056(~52%)  |    3792       |     538\n",
    "    Jade Harley      |   25657    |  3659(~28%)    |   10320(~31%)  |  11590(~41%)  |    2884       |     273\n",
    "    Rose Lalonde     |   25569    |  7170(~14%)    |    7944(~40%)  |  10543(~45%)  |    4505       |     343\n",
    "\n",
    "There was a heavy imbalance of words being said by each character, with Rose saying the least, and Dave saying the most. This was interesting, because Rose had the second-highest number of unique words, which makes sense because of her sizeable vocabulary. As Rose turned out to have one of the highest accuracy when testing the classifier later on, I now realize that a higher percentage of unique words over total words must be correlated with classifier accuracy, as it's easier to identify characters who say more rare words, compared to a character who keeps similar turns of phrase as their friend.\n",
    "\n",
    "\n",
    "The table above shows percentages in reference to the total number of words said by the character. I had originally planned on training on Acts 1-4, development on Act 5, and test on Act 6, but I hadn't realized the sheer disproportionality of the act structure. So instead, I processed the entire comic into one folder per character, and used sk-learn to divide the data.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n"
     ]
    }
   ],
   "source": [
    "#bring on over our nice lil lists\n",
    "ds = dswords\n",
    "je = jewords\n",
    "jh = jhwords\n",
    "rl = rlwords\n",
    "allexamples = ds + je + jh + rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sklearn \n",
    "- load cleaned up data into 'examples'\n",
    "- split 'examples' into training and testing sets\n",
    "    - examples: 'x_train', 'x_test'\n",
    "    - classes: 'c_train', 'c_test'\n",
    "- use countVectorizer \n",
    "    - remove common words with stop_words\n",
    "    - convert text docs to matrix of token counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# load in example files using strategic foldering\n",
    "classes = ['dave', 'john', 'jade', 'rose']\n",
    "examples = sklearn.datasets.load_files(\"examples/\",description=None, categories=classes,load_content=True, encoding='utf-8', shuffle=True, random_state=42)\n",
    "\n",
    "# split into train and test sets; hold out 20% of the dataset for testing/dev\n",
    "x_train, x_test, c_train, c_test = train_test_split(examples.data,examples.target, test_size=0.2)\n",
    "\n",
    "# sklearn: countvec counts tokens, gets rid of stop words\n",
    "countvec = CountVectorizer(stop_words='english')\n",
    "# get counts of training file\n",
    "x_train_counts = countvec.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting features with TF-IDF\n",
    "- term frequency - inverse document frequency\n",
    "- weight the features based on their relative rarity/uniqueness\n",
    "- sklearn: TfidfTransformer on vector created with countvec \n",
    "- referenced https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#tfidf = TfidfTransformer(use_idf=True)\n",
    "#x_train_tfidf = tfidf.fit_transform(x_train_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 Weighted Features per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top10(, clf, class_labels):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" & (class_label, \" \".join(feature_names[j] for j in top10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Classifier\n",
    "\n",
    "#### Stochastic Gradient Descent (SGD) Classifier\n",
    "- calculates loss from one example, rather than total loss from whole set (as in standard)\n",
    "- uses this loss to nudge weight of feature - decreasing learning rate\n",
    "- chosen because it's easy to implement using sklearn and fairly accurate\n",
    "\n",
    "- loss='hinge': default loss function, also fastest one I tested\n",
    "- penalty=l2 for linear SVM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        ...dom_state=51, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "# use pipeline to set up vector representations/freq -> tfidf -> classifier \n",
    "clf = Pipeline([('vect', CountVectorizer(stop_words='english')),('tfidf', TfidfTransformer(use_idf=True)),('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=51,verbose=0)),])\n",
    "# pass training data to fit function\n",
    "clf.fit(x_train, c_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics / Investigate Results\n",
    "\n",
    "\n",
    "- sklearn has some metrics capabilities built in - shown below\n",
    "- results show around a 70% average accuracy\n",
    "- accuracy of characters in descending order\n",
    "    - Rose: 75-86%\n",
    "    - Dave: 72-78%\n",
    "    - John: 66-75%\n",
    "    - Jade: 66-73%\n",
    "- I am not entirely surprised by this order, I believe it is largely because of the difference in vocabulary that Rose and Dave use, versus John and Jade. Using tf-idf got rid of a lot of words overall. Also, this is very specific, but Jade tends to draw out words: \"okay!\" becomes \"oookkaaaayy!!!\" which isn't something that's handled easily by the libraries, but most likely could be mitigated.\n",
    "- If I continue work on this, an enhancement I would consider would be using bi-grams, especially for newline situations\n",
    "- Using NLTK's TwitterTokenizer preserved a lot more than the standard tokenizer, but I still wish I had been able to keep emojis or certain punctuation-based quirks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Success with Classifier on Test Data:  0.6721311475409836\n",
      "\n",
      "SGD Classifier Metrics:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        dave       0.68      0.75      0.71        73\n",
      "        jade       0.70      0.55      0.62        67\n",
      "        john       0.65      0.76      0.70       111\n",
      "        rose       0.69      0.54      0.60        54\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       305\n",
      "   macro avg       0.68      0.65      0.66       305\n",
      "weighted avg       0.68      0.67      0.67       305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nparray to store classifier's predictions of the test set\n",
    "predict = clf.predict(x_test)\n",
    "avgsuccess = (np.mean(predict == c_test))\n",
    "print (\"Average Success with Classifier on Test Data: \", avgsuccess)\n",
    "\n",
    "# check out metrics classification report\n",
    "print('\\nSGD Classifier Metrics:\\n')\n",
    "print(metrics.classification_report(c_test, predict, target_names = examples.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Class 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Jade'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prediction(text):\n",
    "    c = ['Dave', 'John', 'Jade', 'Rose']\n",
    "    p = clf.predict(text)[0]\n",
    "    print(\"Prediction: Class\", p)\n",
    "    return c[p.astype(int)]\n",
    "\n",
    "text = [\"thank you for looking at my presentation\"]\n",
    "prediction(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Class 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Jade'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"i have to give to him\"]\n",
    "prediction(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer\n",
    "class_labels = classes\n",
    "\n",
    "def print_top10(vectorizer, clf, class_labels):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" & (class_label, \" \".join(feature_names[j] for j in top10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_feature_names() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-a27c359b0341>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint_top10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-c50a3d384850>\u001b[0m in \u001b[0;36mprint_top10\u001b[1;34m(vectorizer, clf, class_labels)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclass_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprint_top10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_label\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mtop10\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_feature_names() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "print_top10(vectorizer, clf, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-156e7529d5a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mClassifierMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRegressorMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mClusterMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_params' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin, ClusterMixin, TransformerMixin\n",
    "\n",
    "get_params(self, deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-grams for concat/ phrases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
